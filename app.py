import wget
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_classic.chains import create_retrieval_chain
from langchain_community.document_loaders import TextLoader
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import PromptTemplate
from huggingface_hub import HfFolder

import gradio as gr

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


def get_llm():
    llm = ChatOllama(model="llama3.2", temperature=0.7)
    return llm

def document_loader(file):
    loader = PyPDFLoader(file)
    loaded_document = loader.load()
    return loaded_document

def text_splitter(data):
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    chunks = text_splitter.split_documents(data)
    return chunks

## Embedding model
def embedding():
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embedding

## Vector db
def vector_database(chunks):
    embedding_model = embedding()
    vectordb = Chroma.from_documents(chunks, embedding_model)
    return vectordb

def retriever(file):
    splits = document_loader(file)
    chunks = text_splitter(splits)
    vectordb = vector_database(chunks)
    retriever = vectordb.as_retriever()
    return retriever

qa_prompt = PromptTemplate(
    input_variables=["context", "input"],  # <-- change "question" to "input"
    template="Use the following context to answer the question.\n\nContext: {context}\n\nQuestion: {input}\nAnswer:"
)


def retriever_qa(file, query):
    llm = get_llm()
    retriever_obj = retriever(file)
    combine_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)
    qa = create_retrieval_chain(
        retriever=retriever_obj,
        combine_docs_chain=combine_chain
    )
    response = qa.invoke({"input": query})
    return response.get('answer') or response.get('result') or response.get('text')

rag_application = gr.Interface(
    fn=retriever_qa,
    inputs=[
        gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),
        gr.Textbox(label="Input Query", lines=2, placeholder="Type your question here...")
    ],
    outputs=gr.Textbox(label="Answer"),
    title="Document QA Chatbot",
    description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)


rag_application.launch(server_name="127.0.0.1", server_port= 7860)